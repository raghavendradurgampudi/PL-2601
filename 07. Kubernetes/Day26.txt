Module 6: Kubernetes | Day 26 | 13-02-2026
============================================================================================================
#To delete the cluster
eksctl delete cluster --name kastro-cluster --region ap-south-1

Pod Scheduling Strategies;
=> Node Affinity
A more advanced and flexible version of NodeSelector
soft rule & hard rule
Supports operators like In, NotIn, Exists...

soft rule		- preferred rule	- pod should run on the selected worker node if possible, but if not they can also get scheduled on elsewhere
hard rule	- required rule	- pod must run on the selected worker node only, else it wont schedule

preferredDuringSchedulingIgnoredDuringExecution --- soft rule
requiredDuringSchedulingIgnoredDuringExecution --- hard rule

"In"
the node must have the label key and its value must match one of the given values
label = dev

-key: env
operator: In
values:
 - test
 - dev
 - prod

"NotIn"
the node must have the label key, but its value must not be in the given values
label = dev

-key: env
operator: NotIn
values:
 - test
 - prod

"Exists"
the node must have the label key, regardless of its value

-key: env
operator: Exists

=> Pod Affinity & Pod Anti Affinity
It decides scheduling based on where other pods are available

Pod Affinity - schedule a pod close to other pods
swiggy frontend pods
swiggy backend pods

Pod Anti-Affinity - schedule a pod away from other pods
swiggy frontend pods - worker node 1
Zomato frontend pods

=> Ingress Controller
Microservice Based App
	app1	- deployment yml file, service yml file	- load balancer URL
	app2	- deployment yml file, service yml file	- load balancer URL
	app3	- deployment yml file, service yml file	- load balancer URL

To achieve path based routing, we will work with ingress controller
	LB URL/app1	--- app1
	LB URL/app2	--- app2
	LB URL/app3	--- app3

Types of Ingress controllers;
nginx ingress controller
ingress nginx controller
AWS load balancer controller
HAProxy ingress controller
Kong ingress controller....

Install ingress controller ---->
To install ingress, firstly we have to install nginx ingress controller:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/cloud/deploy.yaml

kubectl get ing --> shows ingress service , no ingress service

Verify the ingress pods
kubectl get pods -n ingress-nginx

Verify the ingress service 
kubectl get svc -n ingress-nginx ----> You will see load balancer url
aaf62657d0dc04b35be250af4d6e47f7-1092281608.ap-south-1.elb.amazonaws.com
We will use the above URL to access the app

Create the directory structure as below;
.
â”œâ”€â”€ app1
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ index.html
â”œâ”€â”€ app2
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ index.html
â”œâ”€â”€ app3
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ index.html
â””â”€â”€ k8s
    â”œâ”€â”€ app1-deploy.yaml
    â”œâ”€â”€ app2-deploy.yaml
    â”œâ”€â”€ app3-deploy.yaml
    â””â”€â”€ ingress.yaml

Dockerfile is same in all the directories;
FROM nginx:alpine
COPY ./index.html /usr/share/nginx/html/index.html

app1/index.html file content ---->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>App 1 - Ingress Demo</title>
  <style>
    body {
      margin: 0;
      background: linear-gradient(135deg, #ff4e50, #f9d423);
      font-family: 'Segoe UI', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      color: #fff;
      text-align: center;
      animation: fadeIn 1s ease-in;
    }
    h1 {
      font-size: 4rem;
      margin-bottom: 10px;
      text-shadow: 2px 2px 8px rgba(0,0,0,0.3);
    }
    p {
      font-size: 1.5rem;
      max-width: 600px;
      line-height: 1.5;
    }
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(-10px); }
      to { opacity: 1; transform: translateY(0); }
    }
  </style>
</head>
<body>
  <div>
    <h1>ðŸš€ App 1 Loaded</h1>
    <p>This is the futuristic red version of App 1, powered by Kubernetes & Ingress on AWS EKS.</p>
  </div>
</body>
</html>


app2/index.html file content ---->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>App 2 - Ingress Demo</title>
  <style>
    body {
      margin: 0;
      background: linear-gradient(135deg, #56ab2f, #a8e063);
      font-family: 'Helvetica Neue', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      color: #fff;
      text-align: center;
      animation: slideUp 1s ease-in-out;
    }
    h1 {
      font-size: 3.5rem;
      margin-bottom: 15px;
    }
    p {
      font-size: 1.4rem;
    }
    @keyframes slideUp {
      from { opacity: 0; transform: translateY(20px); }
      to { opacity: 1; transform: translateY(0); }
    }
  </style>
</head>
<body>
  <div>
    <h1>ðŸŒ¿ App 2 is Live</h1>
    <p>This is App 2, deployed in Kubernetes and beautifully routed via Ingress.</p>
  </div>
</body>
</html>


app3/index.html file content ---->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>App 3 - Ingress Demo</title>
  <style>
    body {
      margin: 0;
      background: linear-gradient(135deg, #1e3c72, #2a5298);
      font-family: 'Roboto', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      color: #ffffffcc;
      text-align: center;
      animation: zoomIn 1s ease;
    }
    h1 {
      font-size: 4rem;
      margin-bottom: 10px;
    }
    p {
      font-size: 1.3rem;
    }
    @keyframes zoomIn {
      from { opacity: 0; transform: scale(0.95); }
      to { opacity: 1; transform: scale(1); }
    }
  </style>
</head>
<body>
  <div>
    <h1>ðŸŒŠ Welcome to App 3</h1>
    <p>This is App 3 â€” clean, responsive, and routed with Kubernetes Ingress on EKS.</p>
  </div>
</body>
</html>


===========
=> AWS Load Balancer Controller

Extract the OIDC ID from your cluster:
oidc_id=$(aws eks describe-cluster --name $cluster_name --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
echo $oidc_id

Check if an IAM OIDC provider already exists for your cluster:
aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4
If output is returned: You already have an IAM OIDC provider and can skip the next step.

If no output is returned: Create an IAM OIDC provider for your cluster:
eksctl utils associate-iam-oidc-provider --cluster kastro-cluster --approve --region us-east-1
aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4
You will see the output, the same OIDC provider will also be there in EKS Console
Lets verify;
EKS -----> Open the Cluster ----> Under "Overview" you will see "OpenID Connect provider URL" ----> You can check the OIDC 

Lets configure aws load balancer controller;
Download the IAM policy required for the AWS Load Balancer Controller:
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json
ls
You will see a file called "iam_policy.json"

Create the IAM policy:
aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json

The name of the policy which we have created is "AWSLoadBalancerControllerIAMPolicy"
The same policy will be created in IAM console. Goto IAM console and verify. Copy the ARN of the policy created.
Ex: "Arn": arn:aws:iam::560185625463:policy/AWSLoadBalancerControllerIAMPolicy

3.2 â€“ Create the IAM Service Account
Create the IAM service account for the AWS Load Balancer Controller using eksctl. This command attaches the policy to the service account and (if it exists) overrides the existing service account:

eksctl create iamserviceaccount \
    --cluster=<ClusterName> \
    --namespace=kube-system \
    --name=aws-load-balancer-controller \
    --attach-policy-arn=<ARNofThePolicy> \
    --override-existing-serviceaccounts \
    --region <ClusterRegion> \
    --approve

Ex:
eksctl create iamserviceaccount \
    --cluster=kastro-cluster \
    --namespace=kube-system \
    --name=aws-load-balancer-controller \
    --attach-policy-arn=arn:aws:iam::560185625463:policy/AWSLoadBalancerControllerIAMPolicy \
    --override-existing-serviceaccounts \
    --region ap-south-1 \
    --approve

To verify;
kubectl get sa -n kube-system
You will see "aws-load-balancer-controller" got created

3.3 â€“ Install the AWS Load Balancer Controller using HELM
#Install HELM
curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version

Add the Helm repository and update it:
helm repo add eks https://aws.github.io/eks-charts
helm repo update eks

Install the AWS Load Balancer Controller:
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=kastro-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller

3.4 â€“ Verify the Controller Deployment
Confirm that the controller is installed and running:
kubectl get deployment -n kube-system aws-load-balancer-controller

Step 4: Deploy the 2048 Game Application
Below are the four YAML files provided. These files create a dedicated namespace, deploy the 2048 game, create a service, and set up an ingress to expose the application via the ALB.

4.1 â€“ Namespace (namespace.yaml)
apiVersion: v1
kind: Namespace
metadata:
  name: 2048-game

Apply the namespace:
kubectl apply -f namespace.yaml

4.2 â€“ Deployment (deployment.yaml)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: game-2048-deployment
  namespace: 2048-game
  labels:
    app: game-2048
spec:
  replicas: 2
  selector:
    matchLabels:
      app: game-2048
  template:
    metadata:
      labels:
        app: game-2048
    spec:
      containers:
      - name: game-2048
        image: thipparthiavinash/2048-game
        ports:
        - containerPort: 80

Deploy the application:
kubectl apply -f deployment.yaml

4.3 â€“ Service (service.yaml)
apiVersion: v1
kind: Service
metadata:
  name: game-2048-service
  namespace: 2048-game
  labels:
    app: game-2048
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: game-2048

Create the service:
kubectl apply -f service.yaml

4.4 â€“ Ingress (ingress.yaml)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: game-2048-ingress
  namespace: 2048-game
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
spec:
  rules:
  - http:
      paths:
      - path: /*
        pathType: ImplementationSpecific
        backend:
          service:
            name: game-2048-service
            port:
              number: 80

Apply the ingress:
kubectl apply -f ingress.yaml

kastro.learndevops01.click

=> ArgoCD
CD tool for k8s deployments

Jenkins ----> Always monitor the repo ----> If there are any changes in the repo, Jenkins will build the job ----> Updated app can be accessible
Jenkins ----> Always monitor the repo ----> k8s ymls ----> replicas 3 => replicas 4, image: v1 => image: v2

GitOps

Lets setup ArgoCD using HELM;
------------------------------------------------
Install HELM
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version

Install ARGOCD using HELM
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

In production, it is always suggested to create a custom namespace ----> kubectl create namespace argocd ----> Lets install argocd in the namespace 'argocd' ----> helm install argocd argo/argo-cd --namespace argocd ----> kubectl get all -n argocd ----> You will see multiple things which are running ----> Under 'services' you can see 'argo-cd server' and the type as ClusterIP. But to access outside of the cluster, we need Load Balancer. So lets edit this ClusterIP to LoadBalancer ----> For this i will use patch ----> 

EXPOSE ARGOCD SERVER:
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}' ----> kubectl get all -n argocd ----> Now you can see the service called 'argo-cd server' changed to Load Balancer instead of ClusterIP ----> Copy the load balancer url ----> This is one way of getting the loadbalancer url. Another way is to install "jq" (J Query) ----> 

yum install jq -y

kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'
The above command will provide load balancer URL to access ARGO CD

Access the argocd using teh above load balancer url ----> Username: admin, 

TO GET ARGO CD PASSWORD:
------------------------------------------
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d


Project 4: https://www.youtube.com/watch?v=1ecF4lKBlMo&list=PLs-PsDpuAuTfG3gFR5DnVD58kT7JBO97x&index=2&pp=iAQB0gcJCYcKAYcqIYzv

Project 5: https://www.youtube.com/watch?v=3fhvnsNY5fc&list=PLs-PsDpuAuTfG3gFR5DnVD58kT7JBO97x&index=4&t=4484s&pp=iAQB
